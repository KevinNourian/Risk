{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **Home Credit Default Risk Assessment**\n",
    "# <center> **Final Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the project, I compare: Logistic Regression, KNN, Random Forest, XGB and LightGBM and measure their performance using ROC-AUC socres. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\Documents\\AI\\Risk\\riskvenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from feature_engine.imputation import ArbitraryNumberImputer\n",
    "from feature_engine.encoding import WoEEncoder\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "import functions\n",
    "import importlib\n",
    "importlib.reload(functions)\n",
    "\n",
    "import optuna\n",
    "import pickle\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Display**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 300000\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth = 500\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('use_inf_as_na', True)\n",
    "\n",
    "data = pd.read_csv(\n",
    "    r\"C:\\Users\\Dell\\Documents\\AI\\Risk\\Data\\Data\\data 27.csv\",\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "data = data.drop('SK_ID_CURR', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 101\n",
    "target = 'TARGET'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Remove Infinity Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imputation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = ArbitraryNumberImputer(arbitrary_number=-99999)\n",
    "ani.fit(data)\n",
    "data = ani.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = CategoricalImputer(imputation_method='missing', fill_value='UNKNOWN')\n",
    "ci.fit(data)\n",
    "data = ci.transform(data)\n",
    "\n",
    "clean_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_data.drop(target, axis=1)\n",
    "y = clean_data[target]\n",
    "\n",
    "X, y = shuffle(X, y, random_state=random_state)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Logistic Regression, Random Forest, XGB Classifier and LGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: ROC AUC = 0.64 (69.11 minutes)\n",
      "KNN: ROC AUC = 0.52 (5.03 minutes)\n",
      "Random Forest: ROC AUC = 0.74 (107.09 minutes)\n",
      "XGBClassifier: ROC AUC = 0.76 (6.20 minutes)\n",
      "LightGBM: ROC AUC = 0.77 (17.29 minutes)\n"
     ]
    }
   ],
   "source": [
    "columns_to_scale = ['ANNUITY_TO_CREDIT_RATIO', \n",
    "                    'EXT_SOURCE_3',\n",
    "                    'EXT_SOURCE_2',\n",
    "                    'EXT_SOURCE_1',\n",
    "                    'EXT_SOURCE_MEAN',\n",
    "                    'ANNUAL_PAYMENT_TO_CREDIT_RATIO',\n",
    "                    'AGE',\n",
    "                    'YEARS_ID_PUBLISH',\n",
    "                    'AMT_ANNUITY',\n",
    "                    'AMT_GOODS_PRICE',\n",
    "                    'ANNUITY_TO_INCOME_RATIO',\n",
    "                    'YEARS_REGISTRATION',\n",
    "                    'YEARS_LAST_PHONE_CHANGE',\n",
    "                    'YEARS_EMPLOYED_AGE_PRODUCT',\n",
    "                    'INCOME_TO_AGE_RATIO',\n",
    "                    'REGION_POPULATION_RELATIVE',\n",
    "                    'AVG_MAX_DPD',\n",
    "                    'TOTAL_DEBIT',\n",
    "                    'TOTAL_CREDIT_AMT',\n",
    "                    'DEBT_CREDIT_RATIO',\n",
    "                    'AVG_ANNUITY_AMOUNT',\n",
    "                    'AVG_DAYS_DECISION',\n",
    "                    'RANGE_DAYS_FIRST_DUE',\n",
    "                    'RANGE_DAYS_LAST_DUE',\n",
    "                    'SUM_AMT_INSTALMENT',\n",
    "                    'AVG_AMT_INSTALMENT',\n",
    "                    'SUM_AMT_PAYMENT',\n",
    "                    'AVG_AMT_PAYMENT',\n",
    "                    'MAX_AMT_PAYMENT',\n",
    "                    'MIN_AMT_PAYMENT',\n",
    "                    'SUM_AMT_PAYMENT/SUM_AMT_INSTALMENT',\n",
    "                    'MEAN_AMT_PAYMENT-MEAN_AMT_INSTALMENT'\n",
    "                    ]\n",
    "\n",
    "columns_to_encode =  ['ORGANIZATION_TYPE']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scaler', StandardScaler(), columns_to_scale),\n",
    "        ('encoder', WoEEncoder(fill_value=.000001), columns_to_encode)\n",
    "    ],\n",
    "    remainder='passthrough' \n",
    ")\n",
    "\n",
    "\n",
    "lg_model = LogisticRegression(class_weight='balanced', random_state=random_state, max_iter=5000)\n",
    "lg_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lg', lg_model)\n",
    "])\n",
    "\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('knn', knn_model)\n",
    "])\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', random_state=random_state)\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('random_forest', rf_model)\n",
    "])\n",
    "\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb', xgb_model)\n",
    "])\n",
    "\n",
    "\n",
    "lgbm_model = LGBMClassifier(class_weight='balanced', random_state=random_state, verbose=0)\n",
    "lgbm_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lgbm', lgbm_model)\n",
    "])\n",
    "\n",
    "\n",
    "pipelines = {\n",
    "    \"Logistic Regression\": lg_pipeline,\n",
    "    \"KNN\": knn_pipeline,\n",
    "    \"Random Forest\": rf_pipeline,\n",
    "    \"XGBClassifier\": xgb_pipeline,\n",
    "    \"LightGBM\": lgbm_pipeline\n",
    " \n",
    "}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    start_time = time.time()\n",
    "\n",
    "    y_pred_proba = cross_val_predict(pipeline, X, y, cv=10, method='predict_proba')[:, 1]\n",
    "    \n",
    "    roc_auc = roc_auc_score(y, y_pred_proba)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "\n",
    "    print(f\"{name}: ROC AUC = {roc_auc:.2f} ({elapsed_time:.2f} minutes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Optuna (LightGBM)**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM turned out to be the model with the best performance. I use Optuna to tune it's hyperparameters in hopes of getting even better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "woe = WoEEncoder(fill_value=0.0001)\n",
    "woe.fit(clean_data, clean_data[target])\n",
    "encoded_data = woe.transform(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = encoded_data.drop(target, axis=1)\n",
    "y = encoded_data[target]\n",
    "\n",
    "X, y = shuffle(X, y, random_state=random_state)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    param = {\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 1000), \n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 64),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 500),  \n",
    "        'min_gain_to_split': trial.suggest_loguniform('min_gain_to_split', 0.0001, 1.0)   \n",
    "    }\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train_scaled, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_test_scaled, label=y_test, reference=train_data)\n",
    "\n",
    "    gbm = lgb.train(\n",
    "        param,\n",
    "        train_data,\n",
    "        valid_sets=[valid_data],  \n",
    "        num_boost_round=100,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]\n",
    "    )\n",
    "\n",
    "    y_pred = gbm.predict(X_test_scaled, num_iteration=gbm.best_iteration)\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LGBM Pipeline Optuna Optimized**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the hyperparameters identified by Optuna to tune my LGBM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.77\n"
     ]
    }
   ],
   "source": [
    "columns_to_scale = ['ANNUITY_TO_CREDIT_RATIO', \n",
    "                    'EXT_SOURCE_3',\n",
    "                    'EXT_SOURCE_2',\n",
    "                    'EXT_SOURCE_1',\n",
    "                    'EXT_SOURCE_MEAN',\n",
    "                    'ANNUAL_PAYMENT_TO_CREDIT_RATIO',\n",
    "                    'AGE',\n",
    "                    'YEARS_ID_PUBLISH',\n",
    "                    'AMT_ANNUITY',\n",
    "                    'AMT_GOODS_PRICE',\n",
    "                    'ANNUITY_TO_INCOME_RATIO',\n",
    "                    'YEARS_REGISTRATION',\n",
    "                    'YEARS_LAST_PHONE_CHANGE',\n",
    "                    'YEARS_EMPLOYED_AGE_PRODUCT',\n",
    "                    'INCOME_TO_AGE_RATIO',\n",
    "                    'REGION_POPULATION_RELATIVE',\n",
    "                    'AVG_MAX_DPD',\n",
    "                    'TOTAL_DEBIT',\n",
    "                    'TOTAL_CREDIT_AMT', \n",
    "                    'DEBT_CREDIT_RATIO',\n",
    "                    'AVG_ANNUITY_AMOUNT',\n",
    "                    'AVG_DAYS_DECISION',\n",
    "                    'RANGE_DAYS_FIRST_DUE',\n",
    "                    'RANGE_DAYS_LAST_DUE',\n",
    "                    'SUM_AMT_INSTALMENT',\n",
    "                    'AVG_AMT_INSTALMENT',\n",
    "                    'SUM_AMT_PAYMENT',\n",
    "                    'AVG_AMT_PAYMENT',\n",
    "                    'MAX_AMT_PAYMENT',\n",
    "                    'MIN_AMT_PAYMENT',\n",
    "                    'SUM_AMT_PAYMENT/SUM_AMT_INSTALMENT',\n",
    "                    'MEAN_AMT_PAYMENT-MEAN_AMT_INSTALMENT'\n",
    "                    ]\n",
    "\n",
    "columns_to_encode = ['ORGANIZATION_TYPE']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scaler', StandardScaler(), columns_to_scale),\n",
    "        ('encoder', WoEEncoder(fill_value=.000001), columns_to_encode)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "lgbm_model = lgb.LGBMClassifier(boosting_type='goss', \n",
    "                           num_leaves=67, \n",
    "                           max_depth=11, \n",
    "                           learning_rate=0.023663556125518563, \n",
    "                           n_estimators=426,\n",
    "                            min_child_samples=270,\n",
    "                            min_gain_to_split=0.026685985757266675, \n",
    "                           verbose=-1)\n",
    "\n",
    "lgbm_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lgbm', lgbm_model)\n",
    "])\n",
    "\n",
    "pipelines = {\n",
    "    \"lgbm\": lgbm_pipeline,\n",
    "}\n",
    "\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_prob)\n",
    "    print(f\"AUC Score: {auc_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pickle File for Streamlit Deployment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will later use Streamlilt for deployment (See notebook 13.0). Here, I create a Pickle file for the Streamlit application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lgbm_pipeline.pkl', 'wb') as file:\n",
    "    pickle.dump(lgbm_pipeline, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * **Models Tested** — Logistic Regression, KNN Classifier, Random Forest Classifier, XGBoost Classifier, LigthGBM Classifier. \n",
    "> * **Best Performing Model** — The best performing model was LightGBM. \n",
    "> * **Optuna** — After hyperparameter tuning using Optuna, ROC-AUC score for the LightGBM model went up by about 3%, from 74% to 77%. \n",
    "> * **Deployment** — I created a Pickle file for Streamlit deployment of the LGBM model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riskvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
